---
title: "Computational Modeling - Assignment 2"
author: "Josephine, Mie and Tobias"
date: "29/01/2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)

p_load(brms, rethinking, tidyverse)

```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci

N.B. there is a second part at the bottom for next week.

### First part

You want to assess your teachers' knowledge of cognitive science. "These guys are a bunch of drama(turgist) queens, mindless philosophers, chattering communication people and Russian spies. Do they really know CogSci?", you think.

To keep things simple (your teachers should not be faced with too complicated things):
- You created a pool of equally challenging questions on CogSci
- Each question can be answered correctly or not (we don't allow partially correct answers, to make our life simpler).
- Knowledge of CogSci can be measured on a scale from 0 (negative knowledge, all answers wrong) through 0.5 (random chance) to 1 (awesome CogSci superpowers)

This is the data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

Questions:

1. What's Riccardo's estimated knowledge of CogSci? What is the probability he knows more than chance (0.5) [try figuring this out. if you can't peek into chapters 3.1 and 3.2 and/or the slides]?
- First implement a grid approximation (hint check paragraph 2.4.1!) with a uniform prior, calculate the posterior and plot the results
- Then implement a quadratic approximation (hint check paragraph 2.4.2!).
- N.B. for the rest of the exercise just keep using the grid approximation (we'll move to quadratic approximations in two classes)

``` {r 1a grid approximation, echo=FALSE}

### Implement grid apporximation
dens <- 10000  # define density

p_grid <- seq(from = 0, to = 1, length.out = dens)  # define grid
prior <- rep(1, dens)  # define prior
#dens(rbinom(10000, 6, runif(10000, 0, 1))) # test the prior, it looks reasonable. The plot of the uniform prior. This is showing how likely the prior believes each outcome to be (0 - 6 correct answers). In this case the plot seems reasonable due to the fact that there is aproximately the same chance for each answer.

likelihood <- dbinom(3, size = 6, prob = p_grid)  # compute likelihood at each value in grid
unstd.posterior <- likelihood * prior  # compute product of likelihood and prio
posterior <- unstd.posterior / sum(unstd.posterior)  # standardize the posterior, so it sums to 1

# plot the posterior probability
plot(p_grid, posterior,
     type = "b",
     xlab = "Probability of cogsci knowledge",
     ylab = "Density")

# add up posterior probability where p < 0.5, this is equivalent to calculating the probability that Riccardo's knowledge is above chance
sum(posterior[p_grid > 0.5])

```


``` {r 1b quadratic approximations, echo=FALSE}

### Implement quadratic apporximation
cogsci_qa <- rethinking::map(
  alist(
    w ~ dbinom(6,p), # binomial distribution
    p ~ dunif(0,1)),  # uniform prior
  data = list(w = 3))

# display summary of quadratic approximation
precis(cogsci_qa)

# specify the results
w <- 3
n <- 6

# plot the actual distribution
curve(dbeta(x, w + 1, n - w + 1), from = 0 , to = 1)
# plot the quadratic approximation
curve(dnorm(x, 0.5, 0.2), lty = 2, add = TRUE)
```

So from the plots and calculations seen above, we estimate that Riccardo has a 50% chance of having knowledge above chance (assuming that the questions are binary, and therefore chance will be at 50%).
The results of the mean are the same for both quadratic approximation and the grid approximation, however, the curves are slightly different.

2. Estimate all the teachers' knowledge of CogSci. Who's best? Use grid approximation. Comment on the posteriors of Riccardo and Mikkel.
2a. Produce plots of the prior, and posterior for each teacher.

```{r functions, echo=FALSE}

#Creating functions

calc_teacher <- function(n_correct, n_question, prior, length_out = 10000){
  # this function calculate the posterior
  p_grid <- seq(from = 0, to = 1, length.out = length_out)
  likelihood <- dbinom(n_correct, 
                       size = n_question, 
                       prob = p_grid)
  unstd_posterior <- prior * likelihood
  bin_size <- abs(p_grid[1] - p_grid[2])
  posterior <- unstd_posterior/sum(unstd_posterior * bin_size)
  return(list(teacher_posterior = posterior, 
              likelihood = likelihood,
              grid = p_grid))
}

pretty_plot <- function(p_grid, prior, likelihood, posterior, title = " "){
  # define data
  d <- tibble(p_grid = p_grid, 
              prior = prior, 
              likelihood = likelihood,
              posterior = posterior)
  
  # make to long format
  d <- d %>% 
    pivot_longer(cols = c("prior", "likelihood", "posterior"), names_to = "name", values_to = "value")
  
  # make a 
  p <- ggplot(d, aes(x = p_grid, y = value, color = name)) + 
    geom_line() + 
    labs(x = "x", y = "Density", title = title) + 
    theme_bw() + 
    ggplot2::theme(panel.background = element_rect(fill = "white"),
                   panel.border = element_blank()) +
    scale_colour_brewer(palette = "Dark2", direction = 1)
  return(p)
}

subtract_plot <- function(posterior, title = " "){
  # define data
  d <- tibble(p_grid = p_grid, 
              subtract_posterior = posterior)
  
  # make to long format
  d <- d %>% 
    pivot_longer(cols = c("subtract_posterior"), names_to = "name", values_to = "value")
  
  # make a 
  p <- ggplot(d, aes(x = p_grid, y = value, color = name)) + 
    geom_line() + 
    labs(x = "x", y = "Density", title = title) + 
    theme_bw() + 
    ggplot2::theme(panel.background = element_rect(fill = "white"),
                   panel.border = element_blank()) +
    scale_colour_brewer(palette = "Dark2", direction = 1)
  return(p)
}
```

``` {r, echo=FALSE}

#Creating a uniform prior
uniform_prior <- rep(1, 10000)

## Riccardo (3/6)
Riccardo <- calc_teacher(3,6, uniform_prior)
pretty_plot(Riccardo$grid, uniform_prior, Riccardo$likelihood, Riccardo$teacher_posterior, title = "Riccardo")


## Kristian (2/2)
Kristian <- calc_teacher(2,2, uniform_prior)
pretty_plot(Kristian$grid, uniform_prior, Kristian$likelihood, Kristian$teacher_posterior, title = "Kristian")


# Josh (160 / 198)
Josh <- calc_teacher(160,198, uniform_prior)
pretty_plot(Josh$grid, uniform_prior, Josh$likelihood, Josh$teacher_posterior, title = "Josh")


# Mikkel (66 / 132)
Mikkel <- calc_teacher(66,132, uniform_prior)
pretty_plot(Mikkel$grid, uniform_prior, Mikkel$likelihood, Mikkel$teacher_posterior, title = "Mikkel")

```

The posteriors of Riccardo and Mikkel are both centered around the mean of 0.5. However, as Mikkel have answered more questions, we are more certain that his actual knowledge of cogsci is around 0.5, which is evident from the curve of his posterior probability being more narrow than the one for Riccardo.

```{r KT vs JS,echo=FALSE}

JS_pos <- Josh$teacher_posterior/sum(Josh$teacher_posterior) 
KT_pos <- Kristian$teacher_posterior/sum(Kristian$teacher_posterior)

sam_JS <- sample(size = 1e7, x = p_grid, prob = JS_pos, replace = TRUE)
mean(sam_JS)
sd(sam_JS)

sam_KT <- sample(size = 1e7, x = p_grid, prob = KT_pos, replace = TRUE)
mean(sam_KT)
sd(sam_KT)

sum(sam_KT < sam_JS)/1e7 * 100

# gives the % of how often Josh knows more then Kristian

```


To determine who has greater CogSci knowledge, we can assess the plots to indentify which posterior distribution has the maximum a posteriori closest to one. If using this criteria, Kristian has the greatest knowledge of CogSci. However, we also have to take into account the certainty of the model, which we determine from calculating and comparing the standard deviations. From this, the teacher with most knowledge is Josh since he has a SD = 0.03, whereas Kristian's distribution has a SD = 0.19.
By sampling from the posterior distributions from Kristian and Josh, the model estimates that the probability of Josh being smarter than Kristian is 52.3%.


3. Change the prior. Given your teachers have all CogSci jobs, you should start with a higher appreciation of their knowledge: the prior is a normal distribution with a mean of 0.8 and a standard deviation of 0.2. Do the results change (and if so how)?
3a. Produce plots of the prior and posterior for each teacher.


``` {r 3a, echo=FALSE}
#creating a new prior 

p_grid <- seq(from = 0, to = 1, length.out = 10000)
sensibly_centered_at_0.8 <- dnorm(p_grid, 0.8, 0.2)

# Riccardo (3 / 6)
Riccardo_0.8<- calc_teacher(3,6,sensibly_centered_at_0.8)
pretty_plot(Riccardo_0.8$grid, sensibly_centered_at_0.8, Riccardo_0.8$likelihood, Riccardo_0.8$teacher_posterior, title = "Riccardo")


## Kristian (2/2)
Kristian_0.8<- calc_teacher(2,2,sensibly_centered_at_0.8)
pretty_plot(Kristian_0.8$grid, sensibly_centered_at_0.8, Kristian_0.8$likelihood, Kristian_0.8$teacher_posterior, title = "Kristian")


# Josh (160 / 198)
Josh_0.8<- calc_teacher(160,198,sensibly_centered_at_0.8)
pretty_plot(Josh_0.8$grid, sensibly_centered_at_0.8, Josh_0.8$likelihood, Josh_0.8$teacher_posterior, title = "Josh")


# Mikkel (66 / 132)
Mikkel_0.8<- calc_teacher(66,132,sensibly_centered_at_0.8)
pretty_plot(Mikkel_0.8$grid, sensibly_centered_at_0.8, Mikkel_0.8$likelihood, Mikkel_0.8$teacher_posterior, title = "Mikkel")


```

The results change for all teachers. However, the degree of change varies due to the amount of data. Previously, the probability distributions of Mikkel's and Riccardo's knowledge were similar. However, since Riccardo has fewer datapoints his posterior probability is more susceptable to change because of the prior. In this case, it increases his posterior probability to a higher degree than Mikkel. The same applies to Kristian, again due to the few datapoints. Josh's results are consistent, which is sensible since he answered more questions.



4. You go back to your teachers and collect more data (multiply the previous numbers by 100). Calculate their knowledge with both a uniform prior and a normal prior with a mean of 0.8 and a standard deviation of 0.2. Do you still see a difference between the results? Why?

``` {r 4, echo=FALSE}
# Riccardo (300 / 600)
Riccardox100 <- calc_teacher(300,600, uniform_prior)
pretty_plot(Riccardox100$grid, uniform_prior, Riccardox100$likelihood, Riccardox100$teacher_posterior, title = "Riccardo uniform")

Riccardo_0.8<- calc_teacher(300,600,sensibly_centered_at_0.8)
pretty_plot(Riccardo_0.8$grid, sensibly_centered_at_0.8, Riccardo_0.8$likelihood, Riccardo_0.8$teacher_posterior, title = "Riccardo sensible")



## Kristian (200/200)
Kristianx100 <- calc_teacher(200,200, uniform_prior)
pretty_plot(Kristianx100$grid, uniform_prior, Kristianx100$likelihood, Kristianx100$teacher_posterior, title = "Kristian uniform")

Kristian_0.8<- calc_teacher(200,200,sensibly_centered_at_0.8)
pretty_plot(Kristian_0.8$grid, sensibly_centered_at_0.8, Kristian_0.8$likelihood, Kristian_0.8$teacher_posterior, title = "Kristian sensible")



# Josh (16000 / 19800)
Joshx100 <- calc_teacher(16000,19800, uniform_prior)
pretty_plot(Joshx100$grid, uniform_prior, Joshx100$likelihood, Joshx100$teacher_posterior, title = "Josh uniform")

Josh_0.8<- calc_teacher(16000,19800,sensibly_centered_at_0.8)
pretty_plot(Josh_0.8$grid, sensibly_centered_at_0.8, Josh_0.8$likelihood, Josh_0.8$teacher_posterior, title = "Josh sensible")



# Mikkel (6600 / 13200)
Mikkelx100 <- calc_teacher(6600,13200, uniform_prior)
pretty_plot(Mikkelx100$grid, uniform_prior, Mikkelx100$likelihood, Mikkelx100$teacher_posterior, title = "Mikkel uniform")

Mikkel_0.8<- calc_teacher(6600,13200,sensibly_centered_at_0.8)
pretty_plot(Mikkel_0.8$grid, sensibly_centered_at_0.8, Mikkel_0.8$likelihood, Mikkel_0.8$teacher_posterior, title = "Mikkel sensible")


```
After adding more data one can observe changes to the posterior. However, the posteriors are less affected by the prior than previously. For instance, this can be seen on the plots for Kristian and Josh. We also see that when the posteriors of the teachers are further away from the prior of 0.8 (Mikkel and Riccardo), they are more influenced by the prior.


5. Imagine you're a skeptic and think your teachers do not know anything about CogSci, given the content of their classes. How would you operationalize that belief?

If we assume the teachers know nothing, we thereby would expect their knowledge to be at chance level at 0.5 with a standard deviation for instance at 0.2.
The code would look as follows:
```{r}
prior <- dnorm(p_grid, 0.5, 0.2)
```




6. Optional question: Can you estimate the difference between Riccardo's estimated knowledge and that of each of the other teachers? Would you deem it credible (that is, would you believe that it is actually different)?

7. Bonus knowledge: all the stuff we have done can be implemented in a lme4-like fashion using the brms package. Here is an example.
```{r, include=FALSE}
# library(brms)
# 
# d <- data.frame(
#   Correct=c(3,2,160,66),
#   Questions=c(6,2,198,132),
#   Teacher=c("RF","KT","JS","MW"))
# 
# # Model sampling only from the prior (for checking the predictions your prior leads to)
# FlatModel_priorCheck <- brm(Correct|trials(Questions) ~ 1, 
#                  data = subset(d, Teacher=="RF"),
#                  prior = prior("uniform(0,1)", class = "Intercept"),
#                  family = binomial,
#                  sample_prior = "only") # here we tell the model to ignore the data
# 
# # Plotting the predictions of the model (prior only) against the actual data
# pp_check(FlatModel_priorCheck, nsamples = 100)
# 
# # Model sampling by combining prior and likelihood
# FlatModel <- brm(Correct|trials(Questions) ~ 1, 
#                  data = subset(d, Teacher=="RF"),
#                  prior = prior("uniform(0,1)", class = "Intercept"),
#                  family = binomial,
#                  sample_prior = T)
# # Plotting the predictions of the model (prior + likelihood) against the actual data
# pp_check(FlatModel, nsamples = 100)
# 
# # plotting the posteriors and the sampling process
# plot(FlatModel)
# 
# 
# PositiveModel_priorCheck <- brm(Correct|trials(Questions) ~ 1,
#                      data = subset(d, Teacher=="RF"),
#                      prior = prior("normal(0.8,0.2)", 
#                                    class = "Intercept"),
#                      family=binomial,
#                      sample_prior = "only")
# pp_check(PositiveModel_priorCheck, nsamples = 100)
# 
# PositiveModel <- brm(Correct|trials(Questions) ~ 1,
#                      data = subset(d, Teacher=="RF"),
#                      prior = prior("normal(0.8,0.2)", 
#                                    class = "Intercept"),
#                      family=binomial,
#                      sample_prior = T)
# pp_check(PositiveModel, nsamples = 100)
# plot(PositiveModel)
# 
# SkepticalModel_priorCheck <- brm(Correct|trials(Questions) ~ 1, 
#                       data = subset(d, Teacher=="RF"),
#                       prior=prior("normal(0.5,0.01)", class = "Intercept"),
#                       family=binomial,
#                       sample_prior = "only")
# pp_check(SkepticalModel_priorCheck, nsamples = 100)
# 
# SkepticalModel <- brm(Correct|trials(Questions) ~ 1, 
#                       data = subset(d, Teacher=="RF"),
#                       prior = prior("normal(0.5,0.01)", class = "Intercept"),
#                       family = binomial,
#                       sample_prior = T)
# pp_check(SkepticalModel, nsamples = 100)
# plot(SkepticalModel)
```

If you dare, try to tweak the data and model to test two hypotheses:
- Is Kristian different from Josh?
- Is Josh different from chance?

### Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works. N.B. You can choose which prior to use for the analysis of last year's data.

Questions to be answered (but see guidance below):
1- Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models

In frequentist models you could use a Student's t-test, which assumes a Gaussian distribution, to get a p-value indicating whether or not the means are significantly different.

Contrarily, Bayesian models estimate how likely it is that the model has learned something. To check the model, one can apply predictive checks like running simulations or if new data is acquired, one can calculate a new posterior using the previous posterior as a prior.



2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.

This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel: 34 correct answers out of 65 questions

Guidance Tips

1. There are at least two ways of assessing predictions.
2. Last year's results are this year's expectations.
3. Are the parameter estimates changing? (way 1)
4. How does the new data look in last year's predictive posterior? (way 2)

```{r, echo=FALSE}
# New function for plotting old posterior and new posterior
posterior_plot <- function(p_grid, prior, posterior, title = " "){
  # define data
  d <- tibble(p_grid = p_grid, 
              old_posterior = prior, 
              new_posterior = posterior)
  
  # make to long format
  d <- d %>% 
    pivot_longer(cols = c("old_posterior", "new_posterior"), names_to = "name", values_to = "value")
  
  # make a 
  p <- ggplot(d, aes(x = p_grid, y = value, color = name)) + 
    geom_line() + 
    labs(x = "x", y = "Density", title = title) + 
    theme_bw() + 
    ggplot2::theme(panel.background = element_rect(fill = "white"),
                   panel.border = element_blank()) +
    scale_colour_brewer(palette = "Dark2", direction = 1)
  return(p)
}

#1 using the information from previous years and see the difference 

#Riccardo
Riccardo_2020 <- calc_teacher(9,10,Riccardo_0.8$teacher_posterior)
posterior_plot(Riccardo_2020$grid, Riccardo_0.8$teacher_posterior, Riccardo_2020$teacher_posterior, title = "Updated posterior for Riccardo")

subtract_plot(Riccardo_2020$teacher_posterior- Riccardo_0.8$teacher_posterior, "Riccardo")

#Kristian
Kristian_2020 <- calc_teacher(8,12,Kristian_0.8$teacher_posterior)
posterior_plot(Kristian_2020$grid, Kristian_0.8$teacher_posterior, Kristian_2020$teacher_posterior, title = "Updated posterior for Kristian")

subtract_plot(Kristian_2020$teacher_posterior- Kristian_0.8$teacher_posterior, "Kristian")

#Josh
Josh_2020 <- calc_teacher(148,172,Josh_0.8$teacher_posterior)
posterior_plot(Josh_2020$grid, Josh_0.8$teacher_posterior, Josh_2020$teacher_posterior, title = "Updated posterior for Josh")

subtract_plot(Josh_2020$teacher_posterior- Josh_0.8$teacher_posterior, "Josh")


#Mikkel
Mikkel_2020 <- calc_teacher(34,65,Mikkel_0.8$teacher_posterior)
posterior_plot(Mikkel_2020$grid, Mikkel_0.8$teacher_posterior, Mikkel_2020$teacher_posterior, title = "Updated posterior for Mikkel")

subtract_plot(Mikkel_2020$teacher_posterior - Mikkel_0.8$teacher_posterior, "Mikkel")
```


Riccardo:
The posterior changed as we do not have a lot of data and the proportion of correct responses is greater for the new data. This has the implication that the new posterior is slightly shifted towards 1 on the x axis. The distribution is still fairly narrow, meaning that we are most certain that Riccardo's cogsci knowledge is around chance (0.5).

Kristian:
The updated posterior is shifted away from 1, meaning that the model has learned from the new data. This is due to the fact that Kristian only answered 2 questions (both correctly) during the first trial and that the second trial had 12 questions - so the amount of data in the second trial is enough to update the posterior.

Josh:
The updated posterior is almost identical to the old posterior (as can also be seen from the subtracted plot), so the model has not learned from the new data. Josh answered a lot of questions during the first trial, so the amount of questions during the second trial was not enough to change the posterior. Also, the new data is fairly consistent with the old data.

Mikkel:
The posterior for Mikkel has only changed minimally. This is due to the amount of data for the second trial not being enough to change the posterior, as well as the proportion of correct responses in the second trial being somewhat consistent with the first trial.

